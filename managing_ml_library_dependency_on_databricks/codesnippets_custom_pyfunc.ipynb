{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ee61a434-3cd9-412c-881f-218f7c24cd26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Custom MLflow Python Function \n",
    "### `ESM2 Protein Embedding Transformer`\n",
    "\n",
    "\n",
    "<!-- # external ref https://huggingface.co/blog/AmelieSchreiber/protein-optimization-and-design -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2260e01-b3c4-4694-b6df-052bf58403bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2355093b-2cad-43ad-8aae-0d72a4a476b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99ebfb7d-9758-471f-ac4c-2859ca67b2f8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "requirements | update with serving error-troubleshooting"
    }
   },
   "outputs": [],
   "source": [
    "# Create the requirements.txt file\n",
    "requirements = \"\"\"\n",
    "torch==2.1.0\n",
    "transformers==4.34.0\n",
    "accelerate==0.23.0\n",
    "cloudpickle==3.1.1\n",
    "\"\"\"\n",
    "\n",
    "requirements_path = f\"/Volumes/{catalog_name}/{schema_name}/{volume_name}/requirements.txt\"\n",
    "\n",
    "# Save the requirements.txt file to UC volumes\n",
    "with open(requirements_path, \"w\") as f:\n",
    "    f.write(requirements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9896da53-d4d0-4862-b86e-137442752a53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -r {requirements_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "543ee012-af9c-47a9-9eff-8fda1edd20f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d412ab69-a583-4e7a-919c-4da26a925202",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "paths"
    }
   },
   "outputs": [],
   "source": [
    "catalog_name = \"mmt_demos\"  \n",
    "schema_name = \"dependencies\"  \n",
    "volume_name = \"esm_artifacts\"  \n",
    "requirements_path = f\"/Volumes/{catalog_name}/{schema_name}/{volume_name}/requirements.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "517cc54d-b2b9-4545-9e25-14f512d6cfda",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ESMWrapper: Custom wrapper class for the transformer model.\n",
    "load_context: Loads the model and tokenizer from the provided artifacts and sets up the device (CPU or GPU).\n",
    "predict: Takes input sequences, tokenizes them, runs them through the model, and returns the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea20d6a4-48af-4a56-a2a2-e9d45f5ffae7",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Wrap transformer as Custom Pyfunc"
    }
   },
   "outputs": [],
   "source": [
    "# Custom PyFunc wrapper for a large transformer model\n",
    "import mlflow\n",
    "import torch\n",
    "from transformers import AutoModelForMaskedLM, AutoTokenizer, logging\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from mlflow.models.signature import infer_signature\n",
    "\n",
    "# Set the logging level to ERROR to disable verbose messages\n",
    "# logging.set_verbosity_error()\n",
    "\n",
    "# Define a Custom wrapper class for the transformer model\n",
    "class ESMWrapper(mlflow.pyfunc.PythonModel):\n",
    "    # load_context: Loads the model and tokenizer from the provided artifacts and sets up the device (CPU or GPU).\n",
    "    def load_context(self, context):\n",
    "        # Load ESM model from saved files in the artifact\n",
    "        # self.artifacts = context.artifacts\n",
    "\n",
    "        # Determine the device to use (GPU if available, otherwise CPU)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        # Load the tokenizer from the provided artifacts\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(context.artifacts[\"tokenizer\"])\n",
    "        # Load the model from the provided artifacts\n",
    "        self.model = AutoModelForMaskedLM.from_pretrained(context.artifacts[\"model\"])\n",
    "        # Move the model to the appropriate device (GPU or CPU)\n",
    "        self.model.to(self.device)\n",
    "        # Set the model to evaluation mode\n",
    "        self.model.eval()\n",
    "\n",
    "        # Set special tokens if they are not set; ensure that the beginning-of-sequence (bos_token) and separator (sep_token) tokens are set. If they are not already set, they are assigned the value of the cls_token (classification token).\n",
    "        if self.tokenizer.bos_token is None:\n",
    "            self.tokenizer.bos_token = self.tokenizer.cls_token\n",
    "        if self.tokenizer.sep_token is None:\n",
    "            self.tokenizer.sep_token = self.tokenizer.cls_token\n",
    "    \n",
    "    # Define the predict function which takes input sequences, tokenizes them, runs them through the model, and returns the embeddings\n",
    "    def predict(self, context, model_input):\n",
    "        protein_sequences = model_input[\"sequences\"]\n",
    "        results = []\n",
    "        \n",
    "        # Process each sequence\n",
    "        for seq in protein_sequences:\n",
    "            inputs = self.tokenizer(seq, return_tensors=\"pt\")\n",
    "            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs, output_hidden_states=True)\n",
    "                \n",
    "            # Process outputs as needed for your application\n",
    "            embeddings = outputs.hidden_states[-1].mean(dim=1).cpu().numpy()\n",
    "            results.append(embeddings)\n",
    "            \n",
    "        return results\n",
    "\n",
    "# Log the model with explicit dependencies\n",
    "with mlflow.start_run():\n",
    "    # Download and Save Model Components:\n",
    "    model_name = \"facebook/esm2_t33_650M_UR50D\" #https://huggingface.co/facebook/esm2_t33_650M_UR50D\n",
    "    \n",
    "    # Specifies the model name and paths to save the model and tokenizer.\n",
    "    model_path = f\"/Volumes/{catalog_name}/{schema_name}/{volume_name}/tmp_model\"\n",
    "    tokenizer_path = f\"/Volumes/{catalog_name}/{schema_name}/{volume_name}/tmp_tokenizer\"\n",
    "    \n",
    "    # Download the pre-trained model and tokenizer from Hugging Face.\n",
    "    model = AutoModelForMaskedLM.from_pretrained(model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    # Saves the model and tokenizer to the specified paths.\n",
    "    model.save_pretrained(model_path, safe_serialization=False)\n",
    "    tokenizer.save_pretrained(tokenizer_path)\n",
    "    \n",
    "    # Define conda env with necessary dependencies\n",
    "    conda_env = {\n",
    "        \"channels\": [\"defaults\", \"conda-forge\", \"pytorch\"],\n",
    "        \"dependencies\": [\n",
    "            \"python=3.11\", # compute 15.4LTSMLR \n",
    "            \"pip>=22.0.4\",\n",
    "            {\"pip\": [\n",
    "                \"torch==2.1.0\", \n",
    "                \"transformers==4.34.0\",\n",
    "                \"accelerate==0.23.0\",\n",
    "                \"cloudpickle==3.1.1\", #compute 15.4LTSMLR    \n",
    "            ]}\n",
    "        ],\n",
    "        \"name\": \"esm_env\"\n",
    "    }\n",
    "    \n",
    "    # Create a sample input DataFrame to infer the input and output signature of the model.\n",
    "    sample_input = pd.DataFrame({\"sequences\": [\"MKTAYIAKQRQISFVKSHFSRQDILDLWIYHTQGYFPDWQNYG\"]})\n",
    "    \n",
    "    ## Initialize the wrapper and load the context manually for signature inference\n",
    "\n",
    "    # Initialize an instance of the ESMWrapper class.\n",
    "    esm_wrapper = ESMWrapper()\n",
    "    # Manually set the tokenizer and model for the wrapper.\n",
    "    esm_wrapper.tokenizer = tokenizer\n",
    "    esm_wrapper.model = model\n",
    "    # Determine the device (GPU or CPU) and moves the model to the appropriate device.\n",
    "    esm_wrapper.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    esm_wrapper.model.to(esm_wrapper.device)\n",
    "    # Set the model to evaluation mode.\n",
    "    esm_wrapper.model.eval()\n",
    "    \n",
    "    # Use the wrapper to predict the output for the sample input\n",
    "    sample_output = esm_wrapper.predict(None, sample_input)\n",
    "    # Infer the input and output signature of the model using the sample input and output\n",
    "    signature = infer_signature(sample_input, sample_output)\n",
    "    \n",
    "    # Log the model with MLflow, including the artifacts (model and tokenizer paths), Conda environment, signature, and input example.\n",
    "    mlflow.pyfunc.log_model(\n",
    "        artifact_path=\"esm_model\",\n",
    "        python_model=ESMWrapper(),\n",
    "        artifacts={\n",
    "            \"model\": model_path,\n",
    "            \"tokenizer\": tokenizer_path,\n",
    "            \"requirements\": requirements_path  \n",
    "        },\n",
    "        conda_env=conda_env,\n",
    "        signature=signature,\n",
    "        input_example=sample_input,\n",
    "        # Register the model with a specified name.\n",
    "        registered_model_name=f\"{catalog_name}.{schema_name}.esm_protein_model\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94520c11-7f10-4ee6-81c3-8708e1acd6d9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "test"
    }
   },
   "outputs": [],
   "source": [
    "sample_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "333c8624-5b63-4646-a32f-a7a8f118b263",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "REQUIREMENTS from errors"
    }
   },
   "outputs": [],
   "source": [
    "# [9xz5z] [2025-04-07 16:15:05 +0000] 2025/04/07 16:15:05 WARNING mlflow.pyfunc: The version of CloudPickle that was used to save the model, `CloudPickle 2.2.1`, differs from the version of CloudPickle that is currently running, `CloudPickle 3.1.1`, and may be incompatible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e16138ee-bd83-4f02-95b4-561efd27e1f2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "get_model_dependencies"
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "model_uri = f\"models:/{catalog_name}.{schema_name}.esm_protein_model/8\"\n",
    "dependencies = mlflow.pyfunc.get_model_dependencies(model_uri)\n",
    "print(dependencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f595d21-5e05-48d3-8194-a15538f8f6ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!cat /local_disk0/repl_tmp_data/ReplId-19610-f2e2d-7/tmpg3ihn9ch/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1b52c46-54cb-4786-8e65-1fc567bbc513",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "get latest registered model version"
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "def get_latest_model_version(model_name):\n",
    "    mlflow_client = MlflowClient(registry_uri=\"databricks-uc\")\n",
    "    latest_version = 1\n",
    "    for mv in mlflow_client.search_model_versions(f\"name='{model_name}'\"):\n",
    "        version_int = int(mv.version)\n",
    "        if version_int > latest_version:\n",
    "            latest_version = version_int\n",
    "    return latest_version\n",
    "\n",
    "def get_model_uri(model_name):\n",
    "  return f\"models:/{model_name}/{get_latest_model_version(model_name)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "505604d0-0a16-4594-b0c4-16a9d6be69e6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Endpoint Configs"
    }
   },
   "outputs": [],
   "source": [
    "host = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiUrl().get()\n",
    "token = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()\n",
    "\n",
    "workload_type = \"GPU_SMALL\"\n",
    "workload_size = \"Small\"\n",
    "\n",
    "registered_model_name = f\"{catalog_name}.{schema_name}.esm_protein_model\"\n",
    "latest_model_version = get_latest_model_version(registered_model_name)\n",
    "general_model_name = f\"esm_protein_model-{latest_model_version}\" \n",
    "\n",
    "endpoint_base_name = \"esm_protein_model_endpoint\"\n",
    "endpoint_name = f\"{endpoint_base_name}-mmt-mlflowsdk\" # endpoint name\n",
    "\n",
    "print(f\"catalog: {catalog_name}\",  \"schema:\", {schema_name}, \"host:\", host, \"token:\", token, workload_type, workload_size, \"general_model_name:\", general_model_name, \"registered_model_name:\", registered_model_name, \"latest_model_version:\", latest_model_version, \"endpoint_name:\", endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36c1054e-c3bd-47b8-baa0-f4b3cabc1f4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# client.delete_endpoint(endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5559a0b6-a10a-4cee-90db-28b31d3d5d34",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Deploy Custom PyFunc as endpoint"
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.deployments import get_deploy_client\n",
    "\n",
    "client = get_deploy_client(\"databricks\")\n",
    "\n",
    "# Define the full API request payload\n",
    "endpoint_config = {\n",
    "    \"name\": general_model_name,\n",
    "    \"served_models\": [\n",
    "        {                \n",
    "            \"model_name\": registered_model_name,\n",
    "            \"model_version\": latest_model_version,\n",
    "            \"workload_size\": workload_size,  # defines concurrency: Small/Medium/Large\n",
    "            \"workload_type\": workload_type,  # defines compute: GPU_SMALL/GPU_MEDIUM/GPU_LARGE\n",
    "            \"scale_to_zero_enabled\": True\n",
    "        }\n",
    "    ],\n",
    "    \"traffic_config\": {\n",
    "        \"routes\": [\n",
    "            {\n",
    "                \"served_model_name\": general_model_name,\n",
    "                \"traffic_percentage\": 100\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    \"auto_capture_config\": {\n",
    "        \"catalog_name\": catalog_name,\n",
    "        \"schema_name\": schema_name,\n",
    "        # \"table_name_prefix\": endpoint_base_name,\n",
    "        \"enabled\": True\n",
    "    },\n",
    "    \"tags\": {\n",
    "        \"project\": \"esm_protein_model\",\n",
    "        \"team\": \"ml-team\",\n",
    "        \"removeAfter\": \"2025-12-31\",\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create or update the endpoint\n",
    "try:\n",
    "    # Check if endpoint exists\n",
    "    existing_endpoint = client.get_endpoint(endpoint_name)\n",
    "    print(f\"Endpoint {endpoint_name} exists, updating configuration...\")\n",
    "    client.update_endpoint_config(endpoint_name, endpoint_config)\n",
    "except Exception as e:\n",
    "    if \"RESOURCE_DOES_NOT_EXIST\" in str(e):\n",
    "        print(f\"Creating new endpoint {endpoint_name}...\")\n",
    "        client.create_endpoint(endpoint_name, endpoint_config)\n",
    "    else:\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ede782f-17c0-4d40-aa46-bf0c5898fef1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "check endpoint status"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "\n",
    "response = requests.post(\n",
    "    f\"{workspace_url}/api/2.0/serving-endpoints\",\n",
    "    headers={\"Authorization\": f\"Bearer {token}\"},\n",
    "    data=json.dumps(endpoint_config)\n",
    ")\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print(\"Serving endpoint created successfully.\")\n",
    "else:\n",
    "    print(f\"Failed to create serving endpoint: {response.text}\")\n",
    "\n",
    "# Check the status of the endpoint\n",
    "endpoint_status_url = f\"{workspace_url}/api/2.0/serving-endpoints/{endpoint_name}\"\n",
    "\n",
    "max_retries = 6  # Number of times to check (6 times for 1 hour)\n",
    "retry_interval = 600  # 10 minutes in seconds\n",
    "\n",
    "# while True:\n",
    "for attempt in range(max_retries):\n",
    "\n",
    "    status_response = requests.get(\n",
    "        endpoint_status_url,\n",
    "        headers={\"Authorization\": f\"Bearer {token}\"}\n",
    "    )\n",
    "    if status_response.status_code == 200:\n",
    "        state = status_response.json().get(\"state\", {})\n",
    "        ready = state.get(\"ready\", False)\n",
    "        config_update = state.get(\"config_update\", False)\n",
    "        message = state.get(\"message\", \"No status message available\")\n",
    "        pending_config = status_response.json().get('pending_config', {})\n",
    "        served_entities = pending_config.get('served_entities', [{}])\n",
    "        served_entity_state = served_entities[0].get('state', 'Unknown')\n",
    "        \n",
    "        if ready and config_update != 'IN_PROGRESS':\n",
    "            print(\"Serving endpoint is ready.\")\n",
    "            break\n",
    "        else:\n",
    "            print(state)\n",
    "            print(f\"Serving endpoint state: {message}\")\n",
    "            print(f\"Served entity state: {served_entity_state}\")\n",
    "            print(\"Waiting for the serving endpoint config_updates to be ready...\")\n",
    "            time.sleep(retry_interval)  # Wait for 10 minutes before checking again\n",
    "    else:\n",
    "        print(f\"Failed to get endpoint status: {status_response.text}\")\n",
    "        break\n",
    "else:\n",
    "    print(\"Endpoint config_update is still not ready after the maximum number of check retries. Please review the Serving UI to see if there are errors or issues.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1a68cd9-f67d-4f2d-a4a2-12f79d7dcac3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "status_response = requests.get(\n",
    "        endpoint_status_url,\n",
    "        headers={\"Authorization\": f\"Bearer {token}\"}\n",
    "    )\n",
    "\n",
    "status_response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ea7f113-6980-418e-b8d9-c5042b82efd3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "state = status_response.json().get(\"state\", {})\n",
    "state\n",
    "# ready = state.get(\"ready\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ae58f26-6358-4c93-9e47-bd17bc04ac79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(status_response.json()['state'], \n",
    " status_response.json()['state']['ready'],\n",
    " status_response.json()['pending_config']['served_entities'][0]['state']\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b32c44e4-6bf6-4507-b3e9-15c1d08f2cd0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "test inference with endpoint"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# Extract Databricks workspace URL and token using dbutils\n",
    "workspace_url = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiUrl().get()\n",
    "token = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()\n",
    "\n",
    "# Endpoint URL\n",
    "endpoint_url = f\"{workspace_url}/serving-endpoints/{endpoint_name}/invocations\"\n",
    "\n",
    "# Sample input\n",
    "input_data = {\n",
    "    \"dataframe_records\": [\n",
    "        {\"sequences\": \"MKTAYIAKQRQISFVKSHFSRQDILDLWIYHTQGYFPDWQNYG\"}\n",
    "    ]\n",
    "}\n",
    "\n",
    "response = requests.post(\n",
    "    endpoint_url,\n",
    "    headers={\"Authorization\": f\"Bearer {token}\", \"Content-Type\": \"application/json\"},\n",
    "    data=json.dumps(input_data)\n",
    ")\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print(\"Prediction:\", response.json())\n",
    "else:\n",
    "    print(f\"Failed to get prediction: {response.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d4c5581-e59b-43a6-8637-88e89702ccdd",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "format to test on UI"
    }
   },
   "outputs": [],
   "source": [
    "input_data = {\n",
    "    \"dataframe_records\": [\n",
    "        {\"sequences\": \"MKTAYIAKQRQISFVKSHFSRQDILDLWIYHTQGYFPDWQNYG\"}\n",
    "    ]\n",
    "}\n",
    "\n",
    "input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b91b1077-75c1-47ad-ba20-bba0deecf211",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "format to test on UI"
    }
   },
   "outputs": [],
   "source": [
    "json.dumps(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3a574007-df22-4733-95fc-43425ce957d2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "test another?"
    }
   },
   "outputs": [],
   "source": [
    "# https://huggingface.co/blog/AmelieSchreiber/protein-optimization-and-design \n",
    "\n",
    "# MAPLRKTYVLKLYVAGNTPNSVRALKTLNNILEKEFKGVYALKVIDVLKNPQLAEEDKILATPTLAKVLPPPVRRIIGDLSNREKVLIGLDLLYEEIGDQAEDDLGLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "199444c3-87b8-48e8-a270-a150ec8cb3a4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "format input json"
    }
   },
   "outputs": [],
   "source": [
    "input_data2 = {\n",
    "    \"dataframe_records\": [\n",
    "        {\"sequences\": \"MKTAYIAKQRQISFVKSHFSRQDILDLWIYHTQGYFPDWQNYG\"},\n",
    "        {\"sequences\": \"MAPLRKTYVLKLYVAGNTPNSVRALKTLNNILEKEFKGVYALKVIDVLKNPQLAEEDKILATPTLAKVLPPPVRRIIGDLSNREKVLIGLDLLYEEIGDQAEDDLGLE\"}\n",
    "    ]\n",
    "}\n",
    "\n",
    "input_data2, json.dumps(input_data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b7b2556e-0ce2-4927-8beb-aebce674f25c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85d4ee1f-feb8-4c5d-81d9-48ae56a55ef1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "test predictions using serving endpoint api"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# Extract Databricks workspace URL and token using dbutils\n",
    "workspace_url = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiUrl().get()\n",
    "token = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()\n",
    "\n",
    "# Endpoint URL\n",
    "endpoint_url = f\"{workspace_url}/serving-endpoints/{endpoint_name}/invocations\"\n",
    "\n",
    "# Sample input with multiple sequences\n",
    "input_data = {\n",
    "    \"dataframe_records\": [\n",
    "        {\"sequences\": \"MKTAYIAKQRQISFVKSHFSRQDILDLWIYHTQGYFPDWQNYG\"},\n",
    "        {\"sequences\": \"MAPLRKTYVLKLYVAGNTPNSVRALKTLNNILEKEFKGVYALKVIDVLKNPQLAEEDKILATPTLAKVLPPPVRRIIGDLSNREKVLIGLDLLYEEIGDQAEDDLGLE\"}\n",
    "    ]\n",
    "}\n",
    "\n",
    "response = requests.post(\n",
    "    endpoint_url,\n",
    "    headers={\"Authorization\": f\"Bearer {token}\", \"Content-Type\": \"application/json\"},\n",
    "    data=json.dumps(input_data)\n",
    ")\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print(\"Prediction:\", response.json())\n",
    "else:\n",
    "    print(f\"Failed to get prediction: {response.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "28f692be-6836-42a5-b99f-b45df4fef9f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27647b1f-6a5b-4ed0-9b21-ea681745d259",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "batch processing using MLflow UC registered model"
    }
   },
   "outputs": [],
   "source": [
    "## actually slower... \n",
    "\n",
    "import mlflow.pyfunc\n",
    "from pyspark.sql.functions import col, flatten\n",
    "\n",
    "# Load the registered model as a Spark UDF\n",
    "model_name = \"mmt_demos.dependencies.esm_protein_model\"\n",
    "model_version = 8\n",
    "model_uri = f\"models:/{model_name}/{model_version}\"\n",
    "model_udf = mlflow.pyfunc.spark_udf(spark, model_uri=model_uri)\n",
    "\n",
    "# Example Spark DataFrame with sequences\n",
    "df = spark.createDataFrame([\n",
    "    (\"MKTAYIAKQRQISFVKSHFSRQDILDLWIYHTQGYFPDWQNYG\",),\n",
    "    (\"MAPLRKTYVLKLYVAGNTPNSVRALKTLNNILEKEFKGVYALKVIDVLKNPQLAEEDKILATPTLAKVLPPPVRRIIGDLSNREKVLIGLDLLYEEIGDQAEDDLGLE\",)\n",
    "], [\"sequences\"])\n",
    "\n",
    "# Apply the model UDF to the DataFrame\n",
    "df_result = df.withColumn(\"predictions\", \n",
    "                          flatten(model_udf(col(\"sequences\"))) \n",
    "                         )\n",
    "\n",
    "# Display the result\n",
    "display(df_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "94c7fc08-86a0-4ca2-b375-8ab215a19e95",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46e0b582-7e87-4183-b4b5-79a68f3dde57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "endpoint_name\n",
    "# esm_protein_model_endpoint-mmt-mlflowsdk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb8a1a04-7742-4cce-b0fd-ea3b3e45f2c7",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "endpoint as udf"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def get_databricks_token():\n",
    "    return dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()\n",
    "\n",
    "def create_tf_serving_json(data):\n",
    "    return {'inputs': {name: data[name].tolist() for name in data.keys()} if isinstance(data, dict) else data.tolist()}\n",
    "\n",
    "def score_model(dataset):\n",
    "    url = f'https://e2-demo-field-eng.cloud.databricks.com/serving-endpoints/{endpoint_name}/invocations'\n",
    "    token = get_databricks_token()\n",
    "    headers = {'Authorization': f'Bearer {token}', 'Content-Type': 'application/json'}\n",
    "    ds_dict = {'dataframe_split': dataset.to_dict(orient='split')} if isinstance(dataset, pd.DataFrame) else create_tf_serving_json(dataset)\n",
    "    data_json = json.dumps(ds_dict, allow_nan=True)\n",
    "    response = requests.request(method='POST', headers=headers, url=url, data=data_json)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f'Request failed with status {response.status_code}, {response.text}')\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "554bf75a-f233-4e0d-b7af-b37215d91cf9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "score with pandasDF"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Example Spark DataFrame with sequences\n",
    "df = spark.createDataFrame([\n",
    "    (\"MKTAYIAKQRQISFVKSHFSRQDILDLWIYHTQGYFPDWQNYG\",),\n",
    "    (\"MAPLRKTYVLKLYVAGNTPNSVRALKTLNNILEKEFKGVYALKVIDVLKNPQLAEEDKILATPTLAKVLPPPVRRIIGDLSNREKVLIGLDLLYEEIGDQAEDDLGLE\",)\n",
    "], [\"sequences\"])\n",
    "\n",
    "# Convert Spark DataFrame to Pandas DataFrame for batch inferencing\n",
    "df_pd = df.toPandas()\n",
    "\n",
    "# Perform batch inferencing using the score_model function\n",
    "model_score = score_model(df_pd)\n",
    "\n",
    "# Convert predictions back to Spark DataFrame\n",
    "predictions_df = pd.DataFrame(model_score['predictions'])\n",
    "scored_df = spark.createDataFrame(predictions_df)\n",
    "\n",
    "# Display the scored DataFrame\n",
    "display(scored_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1cfbd9f-edcd-4fe7-9d7e-715a46df35e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8157ac9-8b3d-4350-926d-ca2320ce82fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ds_dict = {'dataframe_split': df_pd.to_dict(orient='split')} if isinstance(df_pd, pd.DataFrame) else create_tf_serving_json(df_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "990c4422-666b-41d4-8dd0-c1d2b2ec4f36",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ds_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9d05e5d-a8a6-4c22-b9e8-0882d3d77ce8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data_json = json.dumps(ds_dict, allow_nan=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86331eab-7e36-4409-a2b5-5d05a5b83c16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a6350c3d-eb94-4028-8ae7-394c98ce409e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c02f7d42-f03e-4b79-aeb7-e2361bb503be",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "batch inference with endpoint"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from pyspark.sql.functions import pandas_udf, col, explode\n",
    "from pyspark.sql.types import ArrayType, FloatType, StringType, StructType, StructField\n",
    "\n",
    "# Set your Personal Access Token (PAT) here\n",
    "PAT_TOKEN = dbutils.secrets.get(scope=\"mmt\", key=\"databricks_token\")\n",
    "\n",
    "def create_tf_serving_json(data):\n",
    "    return {'inputs': {name: data[name].tolist() for name in data.keys()} if isinstance(data, dict) else data.tolist()}\n",
    "\n",
    "def score_model(dataset):\n",
    "    url = f'https://e2-demo-field-eng.cloud.databricks.com/serving-endpoints/{endpoint_name}/invocations'\n",
    "    token = PAT_TOKEN\n",
    "    headers = {'Authorization': f'Bearer {token}', 'Content-Type': 'application/json'}\n",
    "    ds_dict = {'dataframe_split': dataset.to_dict(orient='split')} if isinstance(dataset, pd.DataFrame) else create_tf_serving_json(dataset)\n",
    "    data_json = json.dumps(ds_dict, allow_nan=True)\n",
    "    response = requests.request(method='POST', headers=headers, url=url, data=data_json)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f'Request failed with status {response.status_code}, {response.text}')\n",
    "    return response.json()['predictions']\n",
    "\n",
    "# Define the schema for the output\n",
    "schema = StructType([\n",
    "    StructField(\"sequences\", StringType(), True),\n",
    "    StructField(\"predictions\", ArrayType(FloatType()), True)\n",
    "])\n",
    "\n",
    "# Define the Pandas UDF\n",
    "@pandas_udf(schema)\n",
    "def score_model_udf(sequences: pd.Series) -> pd.DataFrame:\n",
    "    results = []\n",
    "    for sequence in sequences:\n",
    "        predictions = score_model(pd.DataFrame({\"sequences\": [sequence]}))[0]\n",
    "        # Flatten the predictions if they are nested lists\n",
    "        if isinstance(predictions[0], list):\n",
    "            scores = [item for sublist in predictions for item in sublist]\n",
    "        scores = np.array(scores).astype(np.float32).tolist()  # Convert to float32 and then to list\n",
    "        results.append({\"sequences\": sequence, \"predictions\": scores})\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Example Spark DataFrame with sequences\n",
    "df = spark.createDataFrame([\n",
    "    (\"MKTAYIAKQRQISFVKSHFSRQDILDLWIYHTQGYFPDWQNYG\",),\n",
    "    (\"MAPLRKTYVLKLYVAGNTPNSVRALKTLNNILEKEFKGVYALKVIDVLKNPQLAEEDKILATPTLAKVLPPPVRRIIGDLSNREKVLIGLDLLYEEIGDQAEDDLGLE\",)\n",
    "], [\"sequences\"])\n",
    "\n",
    "# Apply the Pandas UDF to the DataFrame\n",
    "scored_df = df.withColumn(\"scores\", score_model_udf(col(\"sequences\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b45052c-cf6c-4b52-bc32-4aa657703dc4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "predicted scores"
    }
   },
   "outputs": [],
   "source": [
    "## quite fast\n",
    "\n",
    "display(scored_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6f7835a-9d7b-4a48-bdff-679083697cb3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "extract key-values"
    }
   },
   "outputs": [],
   "source": [
    "display(scored_df.select('scores.*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "53e2620a-7d09-454a-90c9-02f64c5ff38f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3394427869922280,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "codesnippets_custom_pyfunc",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
