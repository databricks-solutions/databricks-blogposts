{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8eee4215-1759-4a42-9e5d-ec6295f41be3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 📋 Summary \n",
    "# \n",
    "#### This Schema Evolution Sessionization example shows how to use Apache Spark's transformWithStateInPandas API for schema evolution in stateful stream processing in PySpark. This notebook focuses on transformWithStateInPandas's ValueState capabilities to manage evolving session state schemas across processor versions while maintaining backward compatibility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5db54ca1-bc13-4656-a380-0ea968841aac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PySpark Streaming Sessionization with Schema Evolution Demo\n",
    "# =============================================================================\n",
    "# This notebook demonstrates:\n",
    "# 1. Stateful stream processing with PySpark\n",
    "# 2. Schema evolution in streaming applications\n",
    "# 3. State store management and persistence\n",
    "# 4. Migration from V1 to V2 processor schemas\n",
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e6f2fda-0389-4fd6-8771-cff015d07764",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Install required library for synthetic data generation\n",
    "!pip install dbldatagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5306770b-8ed8-44de-8a81-42b241de7a9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary modules for system and OS operations\n",
    "import sys, os\n",
    "\n",
    "# Import the init module from the utils package\n",
    "from utils import util\n",
    "\n",
    "# Get and display the project directory for reference\n",
    "projectDir = util.get_project_dir()\n",
    "print(\"project directory :\", projectDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "552c80da-a30c-47df-abc0-3bfce16c84d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Spark Configuration for Stateful Processing\n",
    "# =============================================================================\n",
    "\n",
    "# Configure Spark to use RocksDB as the state store provider\n",
    "# RocksDB provides better performance and reliability for stateful operations\n",
    "spark.conf.set(\n",
    "    \"spark.sql.streaming.stateStore.providerClass\",\n",
    "    \"com.databricks.sql.streaming.state.RocksDBStateStoreProvider\"\n",
    ")\n",
    "\n",
    "# Enable changelog checkpointing for better fault tolerance\n",
    "# This helps with faster recovery after failures by maintaining incremental changes\n",
    "spark.conf.set(\n",
    "    \"spark.sql.streaming.stateStore.rocksdb.changelogCheckpointing.enabled\", \n",
    "    \"true\"\n",
    ")\n",
    "\n",
    "# Use Avro encoding format for state serialization\n",
    "# Avro provides efficient serialization and supports schema evolution\n",
    "spark.conf.set(\n",
    "    \"spark.sql.streaming.stateStore.encodingFormat\", \n",
    "    \"avro\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a2055f3-5c39-4715-be11-1f29e4bf4f37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Output and Checkpoint Path Configuration\n",
    "# =============================================================================\n",
    "\n",
    "# Setup directory paths for checkpoints and outputs\n",
    "# Using shared checkpoint to demonstrate schema evolution across processor versions\n",
    "shared_checkpoint = f\"{projectDir}/sessionization/shared_checkpoint/\"  # Shared state store location\n",
    "v1_output = f\"{projectDir}/sessionization/v1_output/\"                  # V1 processor output\n",
    "v2_output = f\"{projectDir}/sessionization/v2_output/\"                  # V2 processor output\n",
    "\n",
    "print(\"Shared checkpoint:\", shared_checkpoint)\n",
    "print(\"V1 Output:\", v1_output)\n",
    "print(\"V2 Output:\", v2_output)\n",
    "\n",
    "# Clean up any previous run data to ensure clean demo environment\n",
    "# This removes existing checkpoints and outputs from previous executions\n",
    "dbutils.fs.rm(shared_checkpoint, True)  # Remove shared checkpoint directory\n",
    "dbutils.fs.rm(v1_output, True)          # Remove V1 output directory\n",
    "dbutils.fs.rm(v2_output, True)          # Remove V2 output directory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "719d0416-d849-4a94-8915-5df3e6ed1f96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Schema Definitions for Output Data\n",
    "# =============================================================================\n",
    "\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType, IntegerType, LongType,\n",
    "    TimestampType, DoubleType, BooleanType\n",
    ")\n",
    "from pyspark.sql.functions import col, lit, when, count, avg, col, array_contains, array, struct, array\n",
    "\n",
    "# Define output schemas for each processor version\n",
    "# These schemas define the structure of completed session records\n",
    "\n",
    "# V1 Output Schema - Basic sessionization fields\n",
    "V1_OUTPUT_SCHEMA = StructType([\n",
    "    StructField(\"session_id\", StringType(), True),      # Unique session identifier\n",
    "    StructField(\"user_id\", StringType(), True),         # User who owns the session\n",
    "    StructField(\"event_count\", IntegerType(), True),    # Number of events in session\n",
    "    StructField(\"total_revenue\", DoubleType(), True),   # Total revenue generated in session\n",
    "    StructField(\"session_start\", TimestampType(), True), # When session began\n",
    "    StructField(\"schema_version\", StringType(), True)   # Schema version identifier\n",
    "])\n",
    "\n",
    "# V2 Output Schema - Enhanced with additional fields and type evolution\n",
    "V2_OUTPUT_SCHEMA = StructType([\n",
    "    StructField(\"session_id\", StringType(), True),      # Same as V1\n",
    "    StructField(\"user_id\", StringType(), True),         # Same as V1\n",
    "    StructField(\"event_count\", LongType(), True),       # TYPE EVOLUTION: Int → Long\n",
    "    StructField(\"total_revenue\", DoubleType(), True),   # Same as V1\n",
    "    StructField(\"session_start\", TimestampType(), True), # Same as V1\n",
    "    StructField(\"device_type\", StringType(), True),     # NEW: Device used in session\n",
    "    StructField(\"page_category\", StringType(), True),   # NEW: Page category information\n",
    "    StructField(\"schema_version\", StringType(), True),  # Schema version identifier\n",
    "    StructField(\"evolved_from_v1\", BooleanType(), True) # NEW: Evolution tracking flag\n",
    "])\n",
    "\n",
    "print(\"Output schemas defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7000c051-1b5a-4210-a2b2-a661c7a8fd74",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Synthetic Data Generation\n",
    "# =============================================================================\n",
    "\n",
    "# Generate overlapping clickstream data containing both V1 and V2 schema events\n",
    "# The data is designed with overlapping session IDs to demonstrate schema evolution\n",
    "print(\"Generating clickstream data with deterministic overlapping session IDs...\")\n",
    "streaming_data = util.create_demo_data(spark)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0f83d722-8dac-47ad-bf4f-83a60a35a3fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PHASE 1: V1 Schema Processing - Establish Initial State Store\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PHASE 1: STARTING WITH V1 SCHEMA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Filter streaming data to process only V1 events in this phase\n",
    "# This simulates a production system initially running with V1 schema\n",
    "v1_only_df = streaming_data.filter(col(\"schema_version\") == \"v1\")\n",
    "\n",
    "print(\"V1 Schema:\")\n",
    "v1_only_df.printSchema()\n",
    "\n",
    "# Start V1 sessionization using transformWithStateInPandas\n",
    "# This establishes the initial state store with V1 schema format\n",
    "print(\"Starting V1 sessionization to establish state store...\")\n",
    "\n",
    "v1_sessionization_query = v1_only_df \\\n",
    "    .groupBy(\"session_id\") \\\n",
    "    .transformWithStateInPandas(\n",
    "        statefulProcessor=util.processor.SessionizerV1(),   # Use V1 processor implementation\n",
    "        outputStructType=V1_OUTPUT_SCHEMA,                  # Define expected output schema\n",
    "        outputMode=\"append\",                                # Only output new completed sessions\n",
    "        timeMode=\"ProcessingTime\"                           # Use processing time for triggers\n",
    "    )\n",
    "\n",
    "# Write V1 sessionized data to Delta table with checkpointing\n",
    "# The checkpoint location will store the V1 state for later evolution\n",
    "v1_stream_query = v1_sessionization_query.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", shared_checkpoint) \\\n",
    "    .option(\"path\", v1_output) \\\n",
    "    .start()\n",
    "\n",
    "print(\"V1 sessionization query started...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e6d8e257-e5a0-40e5-8201-68d1cefd7faa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Let V1 run for a period to establish state store with multiple sessions\n",
    "import time\n",
    "print(\"Letting V1 run for 60 seconds to establish state store...\")\n",
    "time.sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b82281f0-f92e-4ee5-9f2b-2a327cfb176c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# V1 Results Inspection\n",
    "# =============================================================================\n",
    "\n",
    "# Display sessionized results from V1 processor\n",
    "print(\"=== V1 Sessionized Results ===\")\n",
    "sessions_df = spark.read.format(\"delta\").load(v1_output)\n",
    "sessions_df.createOrReplaceTempView(\"v1_session\")\n",
    "display(sessions_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "386ba649-1528-4c2b-a3ee-2d5373f6071a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# State Store Inspection\n",
    "# =============================================================================\n",
    "\n",
    "# Inspect the state store to see persisted session state\n",
    "# This shows sessions that are still active (not yet completed)\n",
    "print(\"\\n🗄️ STATE STORE INSPECTION:\")\n",
    "state_store_values_df = spark.read.format(\"statestore\") \\\n",
    "      .option(\"operatorId\", \"0\") \\\n",
    "      .option(\"stateVarName\", \"session_state\") \\\n",
    "      .load(shared_checkpoint)\n",
    "    \n",
    "state_count = state_store_values_df.count()\n",
    "print(f\"SESSIONS WITH PERSISTED STATE: {state_count}\")\n",
    "\n",
    "print(\"\\nSTATE STORE CONTENTS:\")\n",
    "display(state_store_values_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9636fd81-881d-42a1-b537-057658d7ecb0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Allow V1 Processing Time and Graceful Shutdown\n",
    "# =============================================================================\n",
    "\n",
    "# Let V1 run for a period to establish state store with multiple sessions\n",
    "import time\n",
    "print(\"Letting V1 run for 60 seconds to establish state store...\")\n",
    "time.sleep(60)\n",
    "\n",
    "# Gracefully stop V1 query while preserving state store\n",
    "# The state remains in the checkpoint for V2 to read and evolve\n",
    "v1_stream_query.stop()\n",
    "print(\"✅ V1 query stopped. State store established with V1 schema.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4417a0fc-b488-4214-9ff2-539ee3690fdb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# V1 Results Analysis\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"V1 RESULTS - INITIAL STATE ESTABLISHED\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Read and analyze V1 results\n",
    "v1_sessions_df = spark.read.format(\"delta\").load(v1_output)\n",
    "v1_count = v1_sessions_df.count()\n",
    "print(f\"V1 Sessions Generated: {v1_count}\")\n",
    "\n",
    "if v1_count > 0:\n",
    "    # Analyze V1 session characteristics\n",
    "    print(\"\\nV1 Schema Analysis:\")\n",
    "    v1_sessions_df.groupBy(\"schema_version\").count().show()\n",
    "\n",
    "    print(\"\\nV1 Sample Sessions:\")\n",
    "    display(v1_sessions_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "849a7a9b-153c-4d17-ae59-8ea644a2baa5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PHASE 2: V2 Schema Processing - Demonstrate Schema Evolution\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PHASE 2: SWITCHING TO V2 SCHEMA - SCHEMA EVOLUTION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Filter for V2 data (includes same session IDs as V1 for evolution demo)\n",
    "# This simulates new events arriving with enhanced schema\n",
    "v2_only_df = streaming_data.filter(col(\"schema_version\") == \"v2\")\n",
    "\n",
    "print(\"V2 Schema (note additional fields):\")\n",
    "v2_only_df.printSchema()\n",
    "\n",
    "# Start V2 sessionization using SAME checkpoint location\n",
    "# This is the key to schema evolution - V2 processor reads existing V1 state\n",
    "# and automatically evolves it to V2 format when processing new events\n",
    "print(\"Starting V2 sessionization with SAME checkpoint (demonstrates evolution)...\")\n",
    "\n",
    "v2_sessionization_query = v2_only_df \\\n",
    "    .groupBy(\"session_id\") \\\n",
    "    .transformWithStateInPandas(\n",
    "        statefulProcessor=util.processor.SessionizerV2(),   # Use V2 processor with evolution logic\n",
    "        outputStructType=V2_OUTPUT_SCHEMA,                  # Enhanced output schema\n",
    "        outputMode=\"append\",                                # Only output completed sessions\n",
    "        timeMode=\"ProcessingTime\"                           # Processing time triggers\n",
    "    )\n",
    "\n",
    "# Write V2 sessionized data to separate output path for comparison\n",
    "# Same checkpoint, different output demonstrates evolution in action\n",
    "v2_stream_query = v2_sessionization_query.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", shared_checkpoint)  # SAME checkpoint as V1\n",
    "    .option(\"path\", v2_output) \\\n",
    "    .start()\n",
    "\n",
    "print(\"V2 sessionization query started with schema evolution...\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ed3b3f10-d98a-416e-bf84-402ec7aedc3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Let V2 run for a period to establish state store with multiple sessions\n",
    "import time\n",
    "print(\"Letting V2 run for 60 seconds to establish state store...\")\n",
    "time.sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "86e0e32a-f90b-4a27-9615-15cd8011280c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# V2 Results Inspection\n",
    "# =============================================================================\n",
    "\n",
    "# Display sessionized results from V2 processor\n",
    "print(\"=== V2 Sessionized Results ===\")\n",
    "v2_sessions_df = spark.read.format(\"delta\").load(v2_output)\n",
    "v2_sessions_df.createOrReplaceTempView(\"v2_session\")\n",
    "display(v2_sessions_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c1eab124-70e2-42a3-b713-54cdab952987",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Allow V2 Processing and Schema Evolution\n",
    "# =============================================================================\n",
    "\n",
    "# Let V2 run to process events and demonstrate schema evolution\n",
    "print(\"Letting V2 run for 60 seconds to demonstrate schema evolution...\")\n",
    "time.sleep(60)\n",
    "\n",
    "# Gracefully stop V2 query\n",
    "v2_stream_query.stop()\n",
    "print(\"✅ V2 query stopped. Schema evolution demonstrated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "314fcb36-5418-441f-8b92-63c3c45982be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Schema Evolution Analysis and Results\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SCHEMA EVOLUTION ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Analyze V2 results to understand schema evolution behavior\n",
    "v2_sessions_df = spark.read.format(\"delta\").load(v2_output)\n",
    "v2_sessions_df.createOrReplaceTempView(\"v2_session_out\")\n",
    "v2_count = v2_sessions_df.count()\n",
    "print(f\"V2 Sessions Generated: {v2_count}\")\n",
    "\n",
    "if v2_count > 0:\n",
    "    # Analyze evolution patterns\n",
    "    print(\"\\nEvolution Analysis:\")\n",
    "    v2_sessions_df.groupBy(\"evolved_from_v1\", \"schema_version\").count().show()\n",
    "    \n",
    "    # Show sessions that were evolved from V1 state\n",
    "    print(\"\\nV2 sessions evolved from V1 state:\")\n",
    "    display(spark.sql(\"select * from v2_session_out where evolved_from_v1 = true\"))\n",
    "    \n",
    "    # Show all V2 sessions with enhanced schema\n",
    "    print(\"\\nAll V2 Sample Sessions (note new fields: device_type, page_category):\")\n",
    "    display(spark.sql(\"select * from v2_session_out\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "520e7e02-8294-4ff2-bae2-bdc30acf05aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# End of Schema Evolution Demo\n",
    "# ============================================================================="
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "sessionization_processor",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
