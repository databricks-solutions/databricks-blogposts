{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "406ed2d6-2898-4248-a8b4-7df4bb04adab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, expr, to_timestamp, lit, current_timestamp, window\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, TimestampType, LongType\n",
    "import time\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fa32e66c-f12b-4895-a0c3-5191d1058560",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Stream Generation for Order-Payment Processing\n",
    "\n",
    "## Data Stream Creation\n",
    "This code creates two simulated data streams in Spark:\n",
    "\n",
    "1. **Orders Stream** - Generates purchase order events at a rate of 2 per second\n",
    "2. **Payments Stream** - Generates payment events that correspond to orders\n",
    "\n",
    "## Stream Characteristics\n",
    "\n",
    "\n",
    "### Payments Stream\n",
    "- Creates payments that mostly match with orders for testing stream joins\n",
    "- Includes deliberate test cases:\n",
    "  - **Normal case**: Standard payments with matching order IDs (default)\n",
    "  - **Missing order**: Payments referencing non-existent orders.\n",
    "  - **Early payments**: Payments that arrive before their corresponding orders.\n",
    "  - **Late orders**: Payments for orders that are too old.\n",
    "  - **Timing variations**:\n",
    "    - Payments with timestamps 5 minutes in the past (testing watermark limits)\n",
    "    - Payments with timestamps 8 minutes in the future (testing join window boundaries)\n",
    "\n",
    "This setup allows testing of stream join behavior, late data handling, and watermark functionality in a controlled environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9177864-968f-40a2-a26f-b277333ca3a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "orders_stream = spark.readStream \\\n",
    "    .format(\"rate\") \\\n",
    "    .option(\"rowsPerSecond\", 2) \\\n",
    "    .option(\"numPartitions\", 1) \\\n",
    "    .load() \\\n",
    "    .withColumn(\"order_id\", expr(\"CONCAT('ord-', CAST(value AS STRING))\")) \\\n",
    "    .withColumn(\"customer_id\", expr(\"CONCAT('cust-', CAST(value % 5 AS STRING))\")) \\\n",
    "    .withColumn(\"amount\", expr(\"RAND() * 100\")) \\\n",
    "    .withColumn(\"order_time\", col(\"timestamp\"))\n",
    "\n",
    "\n",
    "payments_stream = spark.readStream \\\n",
    "    .format(\"rate\") \\\n",
    "    .option(\"rowsPerSecond\", 2) \\\n",
    "    .option(\"numPartitions\", 1) \\\n",
    "    .load() \\\n",
    "    .withColumn(\"payment_id\", expr(\"CONCAT('pmt-', CAST(value AS STRING))\")) \\\n",
    "    .withColumn(\"order_id\", expr(\"CASE \" +\n",
    "                               \"WHEN value % 10 = 0 THEN CONCAT('missing-', CAST(value AS STRING)) \" + # No matching order\n",
    "                               \"WHEN value % 10 = 1 THEN CONCAT('ord-', CAST(value + 5 AS STRING)) \" + # Will arrive before order\n",
    "                               \"WHEN value % 10 = 2 THEN CONCAT('ord-', CAST(value - 100 AS STRING)) \" + # Order too old\n",
    "                               \"ELSE CONCAT('ord-', CAST(value AS STRING)) \" + # Normal case - should join\n",
    "                               \"END\")) \\\n",
    "    .withColumn(\"payment_amount\", expr(\"RAND() * 100\")) \\\n",
    "    .withColumn(\"payment_time\", expr(\"CASE \" +\n",
    "                                  \"WHEN value % 10 = 3 THEN timestamp - INTERVAL 5 MINUTES \" + # Payment too old (beyond watermark)\n",
    "                                  \"WHEN value % 10 = 4 THEN timestamp + INTERVAL 8 MINUTES \" + # Payment too far in future (beyond join window)\n",
    "                                  \"ELSE timestamp \" + # Normal timestamp\n",
    "                                  \"END\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "08f2f664-cc58-4d3c-9510-cb8cc06d08d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Adding Watermarks to Streams\n",
    "\n",
    "## Watermark Configuration\n",
    "Watermarks are essential for handling late data in streaming applications. They define how long to wait for late-arriving data before proceeding with operations like joins and aggregations.\n",
    "\n",
    "Apply a 2-minute watermark to the orders stream\n",
    "This means data arriving more than 2 minutes late (compared to the max event time seen) \n",
    "will be dropped from stateful operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9bd7a09-ed5e-4662-966f-871180335766",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "orders_with_watermark = orders_stream \\\n",
    "    .withWatermark(\"order_time\", \"2 minutes\")\n",
    "\n",
    "payments_with_watermark = payments_stream \\\n",
    "    .withWatermark(\"payment_time\", \"2 minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a7f6d0bf-2e2e-41dc-96b8-56639af8117e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Stream-to-Stream Join with Time Constraints\n",
    "\n",
    "## Joining Order and Payment Streams\n",
    "This code performs a windowed join between the orders and payments streams, ensuring that matching payments occur within a specific time window relative to the order.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68969aaa-5a3c-4348-9ce3-1b9ce87c2e8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "joined = orders_with_watermark.alias('order') \\\n",
    "    .join(\n",
    "        payments_with_watermark.alias('payment'),\n",
    "        expr(\"\"\"\n",
    "            order.order_id = payment.order_id AND\n",
    "            order_time >= payment_time - INTERVAL 5 MINUTES AND\n",
    "            order_time <= payment_time + INTERVAL 5 MINUTES\n",
    "        \"\"\"),\n",
    "        \"inner\"\n",
    "    ).select('order.*', 'payment.payment_id', 'payment.payment_amount', 'payment.payment_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "762b4718-382f-4aea-b927-7a0732cb8b9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "checkpoint_location = \"/tmp/delta/order_payment_join_2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b4877881-0e00-48da-9657-4a632cf4aed7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Writing the Joined Stream to Delta Table\n",
    "\n",
    "## Output Configuration\n",
    "This code defines how the joined stream of orders and payments should be persisted to a Delta table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "efc62b8c-74c1-459a-a722-f342dad68eec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query = joined.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", checkpoint_location) \\\n",
    "    .queryName(\"OrderPaymentJoin\") \\\n",
    "    .toTable(\"gp_uc.test.order_payment_join_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a29a685-4bd6-436b-bb93-475126ac69d8",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": null,
       "filterBlob": "{\"filterGroups\":[],\"syncTimestamp\":1743511291453}",
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from gp_uc.test.order_payment_join_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "165e3006-22af-4df2-95f6-7865464b609b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Exlporing using stateStore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e3b91d0f-fbfa-43b4-9dd4-1c91bb57b94f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Monitoring Stream State Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9388d1f8-cb2e-4cea-bade-2bd24abd89bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.read\n",
    "  .format(\"state-metadata\")\n",
    "  .load(checkpoint_location))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c0274b32-c140-4bac-a309-fda3d13a194f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Inspecting State Store Contents\n",
    "\n",
    "https://docs.databricks.com/aws/en/structured-streaming/read-state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "422a8436-0adc-4d1c-b406-752dadaccbe2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Options joinSide or storename mandatory when accessing state store for joined streams\n",
    "# storename can be retrieved from the state metadata table\n",
    "# joinSide can be \"left\" or \"right\"\n",
    "\n",
    "display(spark.read\n",
    "  .format(\"statestore\")\n",
    "  .option(\"storename\", \"left-keyWithIndexToValue\")\n",
    "  .option(\"batchId\", 1453)\n",
    "  .load(checkpoint_location))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12073ee6-39ad-4eca-b0a5-bf00643fbeff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.read\n",
    "  .format(\"statestore\")\n",
    "  .option(\"storename\", \"right-keyWithIndexToValue\")\n",
    "  .option(\"batchId\", 70)\n",
    "  .load(checkpoint_location))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "662a793e-40bc-4770-9a13-2bc3b83149a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Analyzing Left-Side Join State\n",
    "\n",
    "## Retrieving Order Stream State\n",
    "This code extracts and loads the state information for the left side (orders) of our stream-to-stream join operation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44a5c390-3af9-4ae9-8676-86e0739f72e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load state information for the orders (left) side of the stream join\n",
    "df_left = spark.read \\\n",
    "  .format(\"statestore\") \\\n",
    "  .option(\"storename\", \"left-keyWithIndexToValue\") \\\n",
    "  .option(\"batchId\", 70) \\\n",
    "  .load(checkpoint_location) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e03b7476-75a5-425d-82da-900eaa1634be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Keys that didn't match in the right table either due to a watermark or a join window or a mismatch\n",
    "display(df_left.filter(\"value.matched = false\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c03372f-44f0-4393-92e0-84a19db460d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Analyzing Right-Side Join State\n",
    "Retrieving Payment Stream State\n",
    "This code extracts and loads the state information for the right side (payments) of our stream-to-stream join operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21c70a2b-ce23-4f7f-bf60-c7ff2ab563c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load state information for the payments (right) side of the stream join\n",
    "df_right = spark.read \\\n",
    "  .format(\"statestore\") \\\n",
    "  .option(\"storename\", \"right-keyWithIndexToValue\") \\\n",
    "  .option(\"batchId\", 70) \\\n",
    "  .load(checkpoint_location) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5831ff76-846f-46ac-9c8c-742b792b2b7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# distinct keys in the left table\n",
    "display(df_left.select('key.field0').distinct().count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "86599bd6-13b2-454f-ab0c-2c837d7637ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Detecting Key Distribution Skew\n",
    "\n",
    "## Analyzing Key Distribution\n",
    "This code analyzes the distribution of keys in the left state store (orders) to identify potential skew that could affect join performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16fdb469-0751-4c38-913b-24f88db08870",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Count occurrences of each order_id to identify potential data skew\n",
    "display(df_left.groupBy('key.field0').count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7487deda-191d-4730-b918-164bbfd3e96e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_right.select('key.field0').distinct().count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c145fd8c-7366-49ef-94e0-3ffc7e60d972",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_right.groupBy('key.field0').count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "42b2ac06-2d81-4ae6-84c3-663bddec342a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Comparing Left and Right Join States\n",
    "\n",
    "## Cross-Analyzing State Stores\n",
    "This code performs an outer join between the left (orders) and right (payments) state stores to provide a comprehensive view of all records waiting to be matched from both sides.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "925e6cfc-3b95-4831-8590-f004d04d5980",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Joining left and right state stores to find any mismatches\n",
    "df_joined = df_left.join(df_right, df_left['key.field0'] == df_right['key.field0'], 'outer') \\\n",
    "    .select(df_left['key.field0'].alias('left_key'), df_left['value'].alias('left_value'), \n",
    "            df_right['key.field0'].alias('right_key'), df_right['value'].alias('right_value'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2f24115b-a2e7-4411-8cbd-99f3804f7ccc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Validating Join Conditions and Time Windows\n",
    "This code performs sophisticated analysis on the joined state stores to verify expected matching behavior and identify potential issues in the stream-to-stream join.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17cac664-ce3a-4b62-8e09-5dfc50b49b20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, expr, when, abs\n",
    "\n",
    "# Count how many keys from left are matched with right\n",
    "left_matched_count = df_joined.filter(col('left_key').isNotNull() & col('right_key').isNotNull()).count()\n",
    "\n",
    "# Count how many keys from right are matched with left\n",
    "right_matched_count = df_joined.filter(col('right_key').isNotNull() & col('left_key').isNotNull()).count()\n",
    "\n",
    "# Check value.matched and compare order_time with payment_time if needed\n",
    "# Check if payment_time is less than 5 minutes of order_time\n",
    "df_matched = df_joined.withColumn(\n",
    "    'value_matched', \n",
    "    when(col('left_value.matched') == True, True)\n",
    "    .when(\n",
    "        col('left_value.matched') == False, \n",
    "        abs(expr('unix_timestamp(right_value.payment_time) - unix_timestamp(left_value.order_time)')) < 300\n",
    "    ).otherwise(False)\n",
    ")\n",
    "\n",
    "# Display the results\n",
    "display(df_matched)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "13fc2d56-f16d-49be-b4bf-e5f914e08144",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Identifying Failed Join Conditions\n",
    "\n",
    "## Finding Mismatched Records\n",
    "This code identifies records that have matching keys but fail to join due to time window constraints, providing critical insights into join condition issues.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "efd87cdc-2da2-4a34-ad35-0bbaf0245ebe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Keys matched with right table but didn't stastify the join condition\n",
    "display(df_matched.filter(\"value_matched == False\").filter(\"left_value.matched == False\").select('left_key', 'left_value', 'right_key', 'right_value'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "938daf63-0152-49e3-9408-df3fba378e49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Detecting Payments Without Matching Orders\n",
    "This code identifies payment records in the right-side state store that don't have corresponding order records in the left-side state store.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b5cf2cc-aa62-4993-b862-5c3a82058a00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# No keys exists in left table but exists in right table\n",
    "display(df_matched.filter(\"left_key is null\").select('left_key', 'left_value', 'right_key', 'right_value'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2a9925f0-047d-48ab-ac09-975f976594f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Finding Orders Without Corresponding Payments\n",
    "This code reveals order records that have no matching payment record in the current state stores, which could indicate delayed payments or unpaid orders.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec226e00-0eb9-486b-ba7e-5d5d59d4dc5b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Keys exists in left table and didn't match with right table either right keys dropped due to watermark or didn't arrive at all\n",
    "display(df_matched.filter(\"right_key is null\").select('left_key', 'left_value', 'right_key', 'right_value'))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 990384398385740,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "State Store API Demo",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}